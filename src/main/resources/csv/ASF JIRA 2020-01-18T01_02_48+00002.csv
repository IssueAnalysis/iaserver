Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,�� Original Estimate,�� Remaining Estimate,�� Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Container),Outward issue link (Duplicate),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Problem/Incident),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Regression),Outward issue link (Regression),Outward issue link (Required),Outward issue link (Supercedes),Outward issue link (Supercedes),Outward issue link (Supercedes),Outward issue link (Supercedes),Outward issue link (dependent),Outward issue link (dependent),Outward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Colour),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Flags),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (Hadoop Flags),Custom field (Hadoop Flags),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Severity),Custom field (Severity),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Test and Documentation Plan),Custom field (Testcase included),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Crashing bugs in NameNode when using a valid configuration for `dfs.namenode.audit.loggers`,HDFS-15124,13279452,,Bug,Patch Available,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Critical,,,ctest.team,ctest.team,2020/1/14 22:34,2020/1/17 2:12,2020/1/18 1:02,,2.10.0,,,,,,,,,,,,namenode,,,,,,0,,,"I am using Hadoop-2.10.0.

The configuration parameter `dfs.namenode.audit.loggers` allows `default` (which is the default value) and `org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger`.

When I use `org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger`, namenode will not be started successfully because of an `InstantiationException` thrown from `org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initAuditLoggers`.?

The root cause is that while initializing namenode, `initAuditLoggers` will be called and it will try to call the default constructor of `org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger` which doesn't have a default constructor. Thus the `InstantiationException` exception is thrown.

?

*Symptom*

*$ ./start-dfs.sh*
{code:java}
2019-12-18 14:05:20,670 ERROR org.apache.hadoop.hdfs.server.namenode.FSNamesystem: FSNamesystem initialization failed.java.lang.RuntimeException: java.lang.InstantiationException: org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initAuditLoggers(FSNamesystem.java:1024)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:858)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:677)
at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:674)
at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:736)
at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:961)
at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:940)
at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1714)
at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1782)
Caused by: java.lang.InstantiationException: org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger
at java.lang.Class.newInstance(Class.java:427)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initAuditLoggers(FSNamesystem.java:1017)...
8 more
Caused by: java.lang.NoSuchMethodException: org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger.<init>()
at java.lang.Class.getConstructor0(Class.java:3082)
at java.lang.Class.newInstance(Class.java:412)
... 9 more{code}
?

?

*Detailed Root Cause*

There is no default constructor in `org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger`:?
{code:java}
/** 
 * An {@link AuditLogger} that sends logged com.issue.iaserver.data directly to the metrics
 * systems. It is used when the top service is used directly by the name node 
 */ 
@InterfaceAudience.Private 
public class TopAuditLogger implements AuditLogger {     
  public static finalLogger LOG = LoggerFactory.getLogger(TopAuditLogger.class); 

  private final TopMetrics topMetrics; 

  public TopAuditLogger(TopMetrics topMetrics) {
    Preconditions.checkNotNull(topMetrics, ""Cannot init with a null "" + 
        ""TopMetrics"");
    this.topMetrics = topMetrics; 
  }

  @Override
  public void initialize(Configuration conf) { 
  }
{code}
As long as the configuration parameter `dfs.namenode.audit.loggers` is set to `org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger`, `initAuditLoggers` will try to call its default constructor to make a new instance:?
{code:java}
private List<AuditLogger> initAuditLoggers(Configuration conf) {
? // Initialize the custom access loggers if configured.
? Collection<String> alClasses =
? ? ? conf.getTrimmedStringCollection(DFS_NAMENODE_AUDIT_LOGGERS_KEY);
? List<AuditLogger> auditLoggers = Lists.newArrayList();
? if (alClasses != null && !alClasses.isEmpty()) {
? ? for (String className : alClasses) {
? ? ? try {
? ? ? ? AuditLogger logger;
? ? ? ? if (DFS_NAMENODE_DEFAULT_AUDIT_LOGGER_NAME.equals(className)) {
? ? ? ? ? logger = new DefaultAuditLogger();
? ? ? ? } else {
? ? ? ? ? logger = (AuditLogger) Class.forName(className).newInstance();
? ? ? ? }
? ? ? ? logger.initialize(conf);
? ? ? ? auditLoggers.add(logger);
? ? ? } catch (RuntimeException re) {
? ? ? ? throw re;
? ? ? } catch (Exception e) {
? ? ? ? throw new RuntimeException(e);
? ? ? }
? ? }
? }
{code}
`initAuditLoggers` tries to call the default constructor to make a new instance in:
{code:java}
logger = (AuditLogger) Class.forName(className).newInstance();
{code}
This is very different from the default configuration, `default`, which implements a default constructor so the default is fine.

?

*How To Reproduce*?

The version of Hadoop: 2.10.0
 # Set the value of configuration parameter `dfs.namenode.audit.loggers` to `org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger` in ""hdfs-site.xml""(the default value is `default`)
 # Start the namenode by running ""start-dfs.sh""
 # The namenode will not be started successfully.

{code:java}
<property>
??<name>dfs.namenode.audit.loggers</name>
??<value>org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger</value>
??<description>
????List of classes implementing audit loggers that will receive audit events.
????These should be implementations of org.apache.hadoop.hdfs.server.namenode.AuditLogger.
????The special value ""default"" can be used to reference the default audit
????logger, which uses the configured log system. Installing custom audit loggers
????may affect the performance and stability of the NameNode. Refer to the custom
????logger's documentation for more details.
??</description>
</property>
{code}
?

*How To Fix*

Add a default constructor for `org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger`.

I wrote a patch to add a default constructor for?TopAuditLogger.",,ctest.team,elgoiri,weichiu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Jan/20 02:25;ctest.team;HDFS-15124.000.patch;https://issues.apache.org/jira/secure/attachment/12990931/HDFS-15124.000.patch,16/Jan/20 17:25;ctest.team;HDFS-15124.001.patch;https://issues.apache.org/jira/secure/attachment/12991162/HDFS-15124.001.patch,16/Jan/20 22:56;ctest.team;HDFS-15124.002.patch;https://issues.apache.org/jira/secure/attachment/12991181/HDFS-15124.002.patch,,,,,,,,,,,,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,42:12.5,,,FALSE,,,,,,,,,,,,,,,,,9.22337E+18,,,,,Fri Jan 17 02:12:05 UTC 2020,,,,,,0|z0aig0:,9.22337E+18,,,,,,,,,,,,,,,,,,"14/Jan/20 22:42;weichiu;You shouldn't need to add TopAuditLogger to auditlogger configuration. As long as dfs.namenode.top.enabled is true (default), it is added.","14/Jan/20 23:16;ctest.team;[~weichiu] Thank you for your reply!

Yes. The default value can also add?TopAuditLogger, but most users don't read the src code and don't know it.

If the users want to use?TopAuditLogger and they directly set it to?TopAuditLogger (without understanding the src code), then the namenode will crash.

I wrote a patch to add the default constructor for the?TopAuditLogger which I think can make this part more robust.","14/Jan/20 23:39;elgoiri;In addition to the constructor, the exception for this case is a little verbose and not intuitive.
Is there a way we can improve that too?
One can follow the full stack trace but it is buried...","15/Jan/20 03:01;ctest.team;[~elgoiri]?Thank you for your reply and this is a good point! How about catching InstantiationException in `initAuditLoggers(Configuration conf)`

It will look like:
{code:java}
private List<AuditLogger> initAuditLoggers(Configuration conf) {
  // Initialize the custom access loggers if configured.
  Collection<String> alClasses =
      conf.getTrimmedStringCollection(DFS_NAMENODE_AUDIT_LOGGERS_KEY);
  List<AuditLogger> auditLoggers = Lists.newArrayList();
  if (alClasses != null && !alClasses.isEmpty()) {
    for (String className : alClasses) {
      try {
        AuditLogger logger;
        if (DFS_NAMENODE_DEFAULT_AUDIT_LOGGER_NAME.equals(className)) {
          logger = new DefaultAuditLogger();
        } else {
          logger = (AuditLogger) Class.forName(className).newInstance();
        }
        logger.initialize(conf);
        auditLoggers.add(logger);
      } catch (InstantiationException e) {
        LOG.error(""Instantiation error for "" + className);
        throw new RuntimeException(e);
      } catch (RuntimeException re) {
        throw re;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }
  }
{code}
The log error message can be refined. I can upload another patch for this if this is the right way to do it.","15/Jan/20 05:30;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 45s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 24m 26s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  1s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 43s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  7s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 13m 34s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 12s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 20s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 57s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 53s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 53s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 35s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 59s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 25s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 10s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 13s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}115m 21s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 38s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}180m 15s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.namenode.TestRedudantBlocks |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=19.03.5 Server=19.03.5 Image:yetus/hadoop:c44943d1fc3 |
| JIRA Issue | HDFS-15124 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12990931/HDFS-15124.000.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 778f53fdcbb8 4.15.0-65-generic #74-Ubuntu SMP Tue Sep 17 17:06:04 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / c36f09d |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_232 |
| findbugs | v3.1.0-RC1 |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/28675/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/28675/testReport/ |
| Max. process+thread count | 3894 (vs. ulimit of 5500) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/28675/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.

","15/Jan/20 18:37;elgoiri;Thanks [~ctest.team] for the proposal, yes, I think it makes sense.
Just use the logger style with {}.",15/Jan/20 19:45;weichiu;Please make sure you don't add TopAuditLogger twice by setting dfs.namenode.top.enabled to TopAuditLogger.,16/Jan/20 02:05;ctest.team;[~elgoiri] [~weichiu] Sure. I will upload a new patch to do that. Thank you for your suggestions.,"16/Jan/20 17:32;ctest.team;I uploaded a new patch to do all these changes.

[~elgoiri]?I am sorry I didn't use {} style for the LOG in FSNamesystem.java because in?FSNamesystem.java the LOG is `org.apache.commons.logging.Log` instead of `org.slf4j.Logger`. The?`org.apache.commons.logging.Log` doesn't support
{code:java}
LOG.error(""xxx {}"", ""yyy""){code}
Please let me know if anything else needed. Thank you!","16/Jan/20 17:48;elgoiri;FSNameSystem seems to be using logger, right?
https://github.com/apache/hadoop/blob/a0ff42d7612e744e0bf88aa14078ea3ab6bcce49/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java#L373
It was done by HDFS-12552.","16/Jan/20 19:08;ctest.team;[~elgoiri] Sorry that I was using hadoop-2.10.0 and the?FSNameSystem was using?
{code:java}
public static final Log LOG = LogFactory.getLog(FSNamesystem.class);{code}
[https://github.com/apache/hadoop/blob/e2f1f118e465e787d8567dfa6e2f3b72a0eb9194/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java#L328]

It seems that 3.x.x is using?org.slf4j.Logger but 2.x.x is using?org.apache.commons.logging.Log for FSNanesystem.java.

I will write another patch for trunk and upload it again. Thank you for pointing this out!","16/Jan/20 20:49;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 51s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 19m 55s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  4s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 44s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  8s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 14m 29s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 19s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 15s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  3s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 57s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 57s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 41s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 1 new + 166 unchanged - 0 fixed = 167 total (was 166) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  5s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 14m 33s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 42s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 24s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}134m  8s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 33s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}198m 52s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.TestDecommission |
|   | hadoop.hdfs.server.namenode.snapshot.TestSnapshot |
|   | hadoop.hdfs.server.namenode.TestINodeAttributeProvider |
|   | hadoop.hdfs.TestDatanodeRegistration |
|   | hadoop.hdfs.server.namenode.TestRedudantBlocks |
|   | hadoop.hdfs.server.namenode.TestFSImage |
|   | hadoop.hdfs.server.namenode.TestFsck |
|   | hadoop.hdfs.server.namenode.snapshot.TestSnapshotDeletion |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=19.03.5 Server=19.03.5 Image:yetus/hadoop:c44943d1fc3 |
| JIRA Issue | HDFS-15124 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12991162/HDFS-15124.001.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 8fac4d4b27be 4.15.0-74-generic #84-Ubuntu SMP Thu Dec 19 08:06:28 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / a0ff42d |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_232 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/28684/artifact/out/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/28684/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/28684/testReport/ |
| Max. process+thread count | 2649 (vs. ulimit of 5500) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/28684/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.

",17/Jan/20 01:19;ctest.team;Uploaded one new patch for trunk.,"17/Jan/20 02:12;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 41s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 21m 23s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  6s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 48s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 13s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 15m 11s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 28s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 16s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 11s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  1s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m  1s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 41s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 1 new + 166 unchanged - 0 fixed = 167 total (was 166) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 15m 25s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 45s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 24s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}123m 13s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 32s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}191m 22s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.TestDeadNodeDetection |
|   | hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks |
|   | hadoop.hdfs.server.namenode.TestRedudantBlocks |
|   | hadoop.hdfs.server.datanode.TestNNHandlesBlockReportPerStorage |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=19.03.5 Server=19.03.5 Image:yetus/hadoop:c44943d1fc3 |
| JIRA Issue | HDFS-15124 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12991181/HDFS-15124.002.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 0fb99bc6d2d0 4.15.0-74-generic #84-Ubuntu SMP Thu Dec 19 08:06:28 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 263413e |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_232 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/28687/artifact/out/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/28687/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/28687/testReport/ |
| Max. process+thread count | 2570 (vs. ulimit of 5500) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/28687/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RBF: Print stacktrace when DFSRouter fails to fetch/parse JMX output from NameNode,HDFS-15100,13278086,13241304,Sub-task,Resolved,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,Fixed,aajisaka,aajisaka,aajisaka,2020/1/8 8:11,2020/1/10 4:31,2020/1/18 1:03,2020/1/10 4:18,,,,,,3.3.0,,,,,,,rbf,,,,,,0,supportability,,"When DFSRouter fails to fetch or parse JMX output from NameNode, it prints only the error message. Therefore we had to modify the source code to print the stacktrace of the exception to find the root cause.",,aajisaka,ayushsaxena,elgoiri,hudson,John Smith,tasanuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,01:15.6,,,FALSE,,,,,,,,,,,,,,,,,9.22337E+18,,,,,Fri Jan 10 04:31:43 UTC 2020,,,,,,0|z0aacg:,9.22337E+18,,,,,,,,,3.3.0,,,,,,,,,"08/Jan/20 14:01;elgoiri;Can you show the output of the trace before and after?
I personally prefer single line messages for known exceptions. ","09/Jan/20 02:29;aajisaka;Thanks [~inigoiri] for your comment.

This is the output: https://gist.github.com/aajisaka/33699d0ef825cf587e0bb4a5575c1939
This change is for the errors that we have to find the root cause of and fix, so I'd like to see the detailed information.","09/Jan/20 17:47;elgoiri;A little verbose but I guess that's what we need, let me comment in the PR.","10/Jan/20 04:18;tasanuma;Committed to trunk. Thanks for your contribution, [~aajisaka]. Thanks for your review, [~elgoiri].","10/Jan/20 04:31;hudson;SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #17842 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/17842/])
HDFS-15100. RBF: Print stacktrace when DFSRouter fails to fetch/parse (tasanuma: rev 0315ef844862ee863d646b562ba6d8889876ffa9)
* (edit) hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/FederationUtil.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
